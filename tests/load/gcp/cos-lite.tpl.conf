#cloud-config

write_files:
- path: /etc/systemd/system/node-exporter.service
  content: |
    [Unit]
    Description=Node exporter
    After=network.target

    [Service]
    Type=simple
    Restart=always
    RestartSec=5
    ExecStartPre=wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz -P /run
    ExecStartPre=tar -xzvf /run/node_exporter-1.3.1.linux-amd64.tar.gz -C /run
    ExecStartPre=mv /run/node_exporter-1.3.1.linux-amd64/node_exporter /usr/bin
    ExecStart=/usr/bin/node_exporter \
                --web.disable-exporter-metrics \
                --collector.disable-defaults \
                --collector.diskstats \
                --collector.cpu \
                --collector.filesystem \
                --collector.meminfo
- path: /run/wait-for-prom-ready.sh
  permissions: '0755'
  content: |
    #!/bin/bash
    set -eu

    READY=0
    until [ $READY -eq 1 ]; do
      READY=$(curl -s --connect-timeout 2 --max-time 5 http://localhost/prom/-/ready | grep -F 'Prometheus is Ready.' | wc -l)
      sleep 5
    done
- path: /run/wait-for-grafana-ready.sh
  permissions: '0755'
  content: |
    #!/bin/bash
    set -eu

    READY=0
    until [ $READY -eq 1 ]; do
      READY=$(curl -s --connect-timeout 2 --max-time 5 http://localhost/grafana/-/ready | grep -F 'Found' | wc -l)
      # READY=$(juju status grafana --format=json | jq -r '.applications.grafana."application-status".current' | grep -F 'active' | wc -l)
      sleep 5
    done
- path: /etc/systemd/system/prometheus-stdout-logger.service
  content: |
    [Unit]
    Description=Prometheus stdout logger (to journald)
    After=network.target

    [Service]
    Type=simple
    Restart=always
    RestartSec=5
    ExecStartPre=/run/wait-for-prom-ready.sh
    # Going through `current` directly because microk8s-kubectl.wrapper creates subprocesses which expect a login session
    ExecStart=/snap/microk8s/current/kubectl --kubeconfig /var/snap/microk8s/current/credentials/client.config logs prometheus-0 prometheus -n cos-lite-load-test --follow
- path: /run/cos-lite-pod-top.sh
  permissions: '0755'
  content: |
    #!/bin/bash
    set -eu

    while true; do
      # Going through `current` directly because microk8s-kubectl.wrapper creates subprocesses which expect a login session
      /snap/microk8s/current/kubectl --kubeconfig /var/snap/microk8s/current/credentials/client.config top pod -n cos-lite-load-test --no-headers
      sleep 30
    done
- path: /etc/systemd/system/pod-top-logger.service
  content: |
    [Unit]
    Description=pod top logger (to journald)
    After=network.target

    [Service]
    Type=simple
    Restart=always
    RestartSec=5
    ExecStartPre=/run/wait-for-prom-ready.sh
    ExecStart=/run/cos-lite-pod-top.sh
- path: /run/prom-ingress.yaml
  content: |
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: prom-ingress
      namespace: cos-lite-load-test
      annotations:
        nginx.ingress.kubernetes.io/use-regex: "true"
        nginx.ingress.kubernetes.io/rewrite-target: /$1
    spec:
      ingressClassName: public
      rules:
      - http:
          paths:
          - path: /prom/(.*)
            pathType: Prefix
            backend:
              service:
                name: prometheus
                port:
                  number: 9090
          - path: /loki/(.*)
            pathType: Prefix
            backend:
              service:
                name: loki
                port:
                  number: 3100
          - path: /grafana/(.*)
            pathType: Prefix
            backend:
              service:
                name: grafana
                port:
                  number: 3000
- path: /run/overlay-load-test.yaml
  content: |
    --- # overlay.yaml
    applications:
      scrape-target:
        charm: prometheus-scrape-target-k8s
        scale: 1
        trust: true
        channel: edge
        options:
          targets: ${join(",", [for i in range(NUM_TARGETS): "${AVALANCHE_URL}:${9001 + i}"])}
      scrape-config:
        charm: prometheus-scrape-config-k8s
        scale: 1
        trust: true
        channel: edge
        options:
          scrape_interval: ${SCRAPE_INTERVAL}s
          scrape_timeout: ${max(SCRAPE_INTERVAL - 1, 1)}s
      cos-config:
        charm: cos-configuration-k8s
        scale: 1
        trust: true
        channel: edge
        options:
          git_repo: https://github.com/canonical/avalanche-k8s-operator
          git_branch: feature/fix_dashboards
          grafana_dashboards_path: src/grafana_dashboards

    relations:
    - - scrape-target:metrics-endpoint
      - scrape-config:configurable-scrape-jobs
    - - scrape-config:metrics-endpoint
      - prometheus:metrics-endpoint
    - - cos-config:grafana-dashboards
      - grafana:grafana-dashboard
- path: /run/prom-latest.yaml
  content: |
    --- # overlay.yaml
    applications:
      prometheus:
        resources:
          prometheus-image: "prom/prometheus:v2.32.0"
- path: /run/grafana-subpath.yaml
  content: |
    --- # overlay.yaml
    applications:
      grafana:
        options:
          web_external_url: "/grafana"
- path: /run/cos-lite-rest-server.py
  permissions: '0755'
  content: |
    #!/usr/bin/env python3

    # FLASK_APP=./cos-lite-rest-server.py flask run -p 8081 --host 0.0.0.0

    import functools
    import subprocess
    import yaml
    from flask import Flask

    app = Flask(__name__)

    @app.route("/helper/grafana/password")
    @functools.lru_cache
    def get_admin_password():
        action_result = subprocess.Popen(['juju', 'run-action', 'grafana/0', 'get-admin-password', '--wait'], stdout=subprocess.PIPE, user="ubuntu")
        as_dict = yaml.safe_load(action_result.stdout.read().decode())
        return as_dict["unit-grafana-0"]["results"]["admin-password"]
- path: /etc/systemd/system/cos-lite-rest-server.service
  content: |
    [Unit]
    Description=rest server for making some things easier, such as obtaining the grafana admin password
    After=network.target multi-user.target

    [Service]
    Type=simple
    Restart=always
    RestartSec=5
    Environment="FLASK_APP=/run/cos-lite-rest-server.py"
    ExecStart=/usr/bin/flask run -p 8081 --host 0.0.0.0

package_update: true

packages:
- sysstat
- python3-pip
- jq
- python3-flask

snap:
  commands:
  - snap install --classic juju --channel=latest/candidate
  - snap install --classic microk8s
  - snap alias microk8s.kubectl kubectl
  - snap refresh

runcmd:
- |
  # install deps
  DEBIAN_FRONTEND=noninteractive apt -y upgrade

- |
  # disable swap
  sysctl -w vm.swappiness=0
  echo "vm.swappiness = 0" | tee -a /etc/sysctl.conf
  swapoff -a

- |
  # disable unnecessary services
  systemctl disable man-db.timer man-db.service --now
  systemctl disable apport.service apport-autoreport.service  --now
  systemctl disable apt-daily.service apt-daily.timer --now
  systemctl disable apt-daily-upgrade.service apt-daily-upgrade.timer --now
  systemctl disable unattended-upgrades.service --now
  systemctl disable motd-news.service motd-news.timer --now
  systemctl disable bluetooth.target --now
  systemctl disable ua-messaging.service ua-messaging.timer --now
  systemctl disable ua-timer.timer ua-timer.service --now
  systemctl disable systemd-tmpfiles-clean.timer --now

  # start services
  systemctl daemon-reload
  sed -i 's/ENABLED="false"/ENABLED="true"/' /etc/default/sysstat
  systemctl restart sysstat sysstat-collect.timer sysstat-summary.timer
  systemctl start node-exporter.service

- |
  set -eux
  # setup microk8s and bootstrap
  adduser ubuntu microk8s
  microk8s status --wait-ready
  microk8s enable dns:$(grep nameserver /run/systemd/resolve/resolv.conf | awk '{print $2}')
  microk8s.enable storage ingress
  # wait for addons to become available
  microk8s.kubectl rollout status deployments/hostpath-provisioner -n kube-system -w --timeout=600s
  microk8s.kubectl rollout status deployments/coredns -n kube-system -w --timeout=600s
  microk8s.kubectl rollout status daemonsets/nginx-ingress-microk8s-controller -n ingress -w --timeout=600s
  microk8s.enable metrics-server
  microk8s.kubectl rollout status deployment.apps/metrics-server -n kube-system -w --timeout=600s

  # To prevent metallb from failing with the following error:
  # The connection to the server 127.0.0.1:16443 was refused - did you specify the right host or port?
  # the metallb addon must be enabled only after the dns addon was rolled out
  # https://github.com/ubuntu/microk8s/issues/2770#issuecomment-984346287
  IPADDR=$(ip -4 -j route | jq -r '.[] | select(.dst | contains("default")) | .prefsrc')
  microk8s.enable metallb:$IPADDR-$IPADDR
  microk8s.kubectl rollout status daemonset.apps/speaker -n metallb-system -w --timeout=600s

  # workaround for
  # ERROR resolving microk8s credentials: max duration exceeded: secret for service account "juju-credential-microk8s" not found
  # Ref: https://github.com/charmed-kubernetes/actions-operator/blob/main/src/bootstrap/index.ts
  microk8s.kubectl create serviceaccount test-sa
  timeout 600 sh -c "until microk8s.kubectl get secrets | grep -q test-sa-token-; do sleep 5; done"
  microk8s.kubectl delete serviceaccount test-sa

- |
  set -eux
  # prep juju
  sudo -u ubuntu juju bootstrap --no-gui microk8s uk8s
  sudo -u ubuntu juju add-model --config logging-config="<root>=WARNING; unit=DEBUG" --config update-status-hook-interval="60m" cos-lite-load-test
  sudo -u ubuntu juju deploy --channel=edge cos-lite --trust --overlay /run/overlay-load-test.yaml --overlay /run/prom-latest.yaml --overlay /run/grafana-subpath.yaml --trust

- |
  set -eux
  # ingress must be applied _after_ deployment
  microk8s.kubectl apply -f /run/prom-ingress.yaml

- |
  # install ops agent for observability
  #echo '"projects/${PROJECT}/zones/${ZONE}/instances/${INSTANCE}","[{""type"":""ops-agent""}]"' >> agents_to_install.csv && \
  #curl -sSO https://dl.google.com/cloudagents/mass-provision-google-cloud-ops-agents.py && \
  #python3 mass-provision-google-cloud-ops-agents.py --file agents_to_install.csv

  #curl -sSO https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh
  #sudo bash add-google-cloud-ops-agent-repo.sh --also-install

- |
  # start services
  # Waiting for prom here because systemd would timeout waiting for the unit to become active/idle:
  #   Job for prometheus-stdout-logger.service failed because a timeout was exceeded.
  /run/wait-for-prom-ready.sh
  systemctl start prometheus-stdout-logger.service
  systemctl start pod-top-logger.service

- |
  # wait for grafana to become active
  /run/wait-for-grafana-ready.sh
  systemctl start cos-lite-rest-server.service

  # force reldata reinit in case files appeared on disk after the last hook fired
  sudo -u ubuntu juju run-action cos-config/0 sync-now --wait
